{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучить сеть, попробовать попрунить, сконвертировать в onnx и запустить в юнити."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas\n",
    "import torch\n",
    "\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch import device\n",
    "from torch import load\n",
    "from torch.cuda import is_available\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from show import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_class = 256  # Количество классов в задаче\n",
    "num_epochs = 300  # Количество эпох обучения\n",
    "lr = 0.00008  # Коэффициент скорости обучения (Learning rate)\n",
    "momentum = 0.8\n",
    "momentumB = 0.9\n",
    "step_size = 10\n",
    "gamma = 0.9\n",
    "label_smoothing = 0.0\n",
    "batch_size = 100  # Размер бача\n",
    "snp_path_0 = \"/workspace/prj/snp/\"  # Путь к папке, в которую сохранять готовые модели\n",
    "model_path = None  # \"/workspace/prj/snp/Caltech256/mobilenetV2/30_05_2023/15_58_51/mobilenetV2_Caltech256_292_ACC top1-0.5907_checkpoint.tar\"\n",
    "pretrained = False  # True - загрузить предобученную модель\n",
    "num_workers = 5  # Cколько подпроцессов использовать для загрузки данных\n",
    "pin_memory = True  # Ускорить ли загрузки данных с CPU на GPU False если очень маленький набор данных\n",
    "obj_transforms = (\n",
    "    None  # Аугментации val_transforms, train_transforms на основе Albumentation\n",
    ")\n",
    "SIZE = 224  # Размер входа (SIZE*SIZE)\n",
    "model_name = \"mobilenetV2\"\n",
    "Dataset_name = \"Caltech256\"\n",
    "rasp_file_train = None\n",
    "shuffle_train = False\n",
    "drop_last_train = True\n",
    "rasp_file_val = None\n",
    "shuffle_val = False\n",
    "drop_last_val = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_criterion = nn.CrossEntropyLoss()  # label_smoothing=label_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=10):\n",
    "    \"\"\"Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return random_state\n",
    "\n",
    "\n",
    "random_state = set_seed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageOps\n",
    "import pandas\n",
    "\n",
    "\n",
    "class GetData(Dataset):\n",
    "    def __init__(self, Root, annotation, Valid=False, Transform=None):\n",
    "\n",
    "        self.landmarks_frame = pandas.read_csv(annotation).query(\n",
    "            \"isval == \" + str(Valid)\n",
    "        )\n",
    "        self.transform = Transform\n",
    "        self.root = Root\n",
    "        self.valid = Valid\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(\n",
    "            self.root, str(self.landmarks_frame[\"impath\"].iloc[index])\n",
    "        )\n",
    "        image = cv2.imread(img_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        landmarks = self.landmarks_frame[\"label\"].iloc[index]\n",
    "        landmarks = np.array(landmarks)\n",
    "\n",
    "        return image, landmarks\n",
    "\n",
    "    def getSempler(self, name_class=\"label\"):\n",
    "        rasp_sampler_list = []\n",
    "        class_count = self.landmarks_frame[name_class].value_counts()\n",
    "        rasp_sampler_list = [\n",
    "            1 / class_count[i] for i in self.landmarks_frame[name_class].values\n",
    "        ]\n",
    "\n",
    "        if self.__len__() != len(rasp_sampler_list):\n",
    "            raise ValueError(\"sampler does not converge with the map\")\n",
    "        sempler = WeightedRandomSampler(rasp_sampler_list, len(rasp_sampler_list))\n",
    "        return sempler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneOf:\n",
    "    def __init__(self, transforms, p: float = 0.5):\n",
    "        self.p = p\n",
    "        self.transforms = transforms\n",
    "        transforms_ps = [1 for t in self.transforms]\n",
    "        s = sum(transforms_ps)\n",
    "        self.transforms_ps = [t / s for t in transforms_ps]\n",
    "\n",
    "    def __call__(self, img):\n",
    "\n",
    "        if self.transforms_ps and (random.random() < self.p):\n",
    "            t = random.choices(population=self.transforms, weights=self.transforms_ps)\n",
    "            data = t[0](img)\n",
    "        return data\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        OneOf(\n",
    "            [\n",
    "                transforms.Compose(\n",
    "                    [\n",
    "                        transforms.Resize((int(SIZE), int(SIZE))),\n",
    "                        transforms.RandomCrop((int(SIZE), int(SIZE))),\n",
    "                    ]\n",
    "                ),\n",
    "                transforms.RandomResizedCrop((SIZE, SIZE)),\n",
    "                transforms.Resize((SIZE, SIZE)),\n",
    "            ],\n",
    "            p=1,\n",
    "        ),\n",
    "        OneOf([transforms.AutoAugment(), transforms.RandAugment(3)], p=1),\n",
    "        transforms.ToTensor(),  # преобразуем изображение в тензор\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        ),  # Нормируем получившийся тезор с мат ожиданием и стандартным отклонением для каждого канала тензора\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((SIZE, SIZE)),\n",
    "        transforms.ToTensor(),  # преобразуем изображение в тензор\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        ),  # Нормируем получившийся тезор с мат ожиданием и стандартным отклонением для каждого канала тензора\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_dir = \"/workspace/db_labs/paradigma/Caltech256\"\n",
    "annotation_imagenette = Data_dir + \"/data.csv\"\n",
    "train_dataset = GetData(Data_dir, annotation_imagenette, False, train_transforms)\n",
    "val_dataset = GetData(Data_dir, annotation_imagenette, True, val_transforms)\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "# dataset_test=MyDatasetCaltech256(dirr_path,test_annotation,val_transforms)\n",
    "sampler_train = train_dataset.getSempler()\n",
    "sampler_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_train,\n",
    "        sampler=train_dataset.getSempler(),\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last_train,\n",
    "        pin_memory=pin_memory,\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_val,\n",
    "        sampler=sampler_val,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=drop_last_val,\n",
    "        pin_memory=pin_memory,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    classification_criterion,\n",
    "    optimizer,\n",
    "    dataloaders,\n",
    "    scheduler,\n",
    "    batch_size,\n",
    "    snp_path,\n",
    "    Name_experement,\n",
    "    num_epochs,\n",
    "):\n",
    "    # Запомнить время начала обучения\n",
    "    devices = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    since = time.time()\n",
    "    mass = [[], [], []]\n",
    "    # Копировать параметры поданной модели\n",
    "    best_model_the_loss_classification = model.state_dict()\n",
    "    best_model_the_acc_classification = model.state_dict()\n",
    "    # print(\"GPU inference time: {:8f},CPU inference time:{:8f}\".format(SeachInferensModel(model),SeachInferensModel(model,\"cpu\")))\n",
    "    for epoch in range(num_epochs):\n",
    "        # У каждой эпохи есть этап обучения и проверки\n",
    "\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Установить модель в режим обучения\n",
    "            elif phase == \"val\":\n",
    "                model.eval()  # Установить модель в режим оценки\n",
    "\n",
    "            # Обнуление параметров\n",
    "            running_classification_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            dataset_sizes = 0\n",
    "            # Получать порции картинок и иx классов из датасета\n",
    "\n",
    "            pbar = tqdm(\n",
    "                enumerate(dataloaders[phase]),\n",
    "                total=len(dataloaders[phase]),\n",
    "                desc=\"Epocha \" + phase + \" \" + str(epoch + 1) + \"/\" + str(num_epochs),\n",
    "            )\n",
    "            for step, (inputs, labels) in pbar:\n",
    "\n",
    "                # считать все на видеокарте или ЦП\n",
    "                inputs = inputs.to(devices)\n",
    "                labels = labels.to(devices)\n",
    "                # обнулить градиенты параметра\n",
    "                optimizer.zero_grad()\n",
    "                # Пока градиент можно поcчитать, cчитать только на учимся\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    # Проход картинок через модель\n",
    "                    classification = model(inputs)\n",
    "                    total_classification_loss = classification_criterion(\n",
    "                        classification, labels.to(dtype=torch.long)\n",
    "                    )\n",
    "\n",
    "                    # Если учимся\n",
    "                    if phase == \"train\":\n",
    "                        # Вычислить градиенты\n",
    "                        total_classification_loss.backward()\n",
    "                        # Обновить веса\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Статистика\n",
    "                # for i in range(batch_size):# Колличество правильных ответов\n",
    "                #     running_corrects += float(torch.sum(torch.argmax(classification[i]) == labels[i]))\n",
    "                running_corrects += float(\n",
    "                    torch.sum(\n",
    "                        labels.unsqueeze(1)\n",
    "                        == torch.topk(\n",
    "                            input=classification, k=1, dim=1, largest=True, sorted=True\n",
    "                        )[1]\n",
    "                    )\n",
    "                )\n",
    "                running_classification_loss += (\n",
    "                    total_classification_loss.item() * inputs.size(0)\n",
    "                )\n",
    "                dataset_sizes = dataset_sizes + batch_size\n",
    "\n",
    "                epoch_classification_loss = running_classification_loss / dataset_sizes\n",
    "                epoch_acc = running_corrects / dataset_sizes\n",
    "                mem = (\n",
    "                    torch.cuda.memory_reserved() / 1e9\n",
    "                    if torch.cuda.is_available()\n",
    "                    else 0\n",
    "                )\n",
    "                current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "                pbar.set_postfix(\n",
    "                    valid_loss=f\"{epoch_classification_loss:0.4f}\",\n",
    "                    acc=f\"{epoch_acc:0.5f}\",\n",
    "                    lr=f\"{current_lr:0.5f}\",\n",
    "                    gpu_memory=f\"{mem:0.2f} GB\",\n",
    "                )\n",
    "\n",
    "            if epoch == 0 and phase == \"train\":\n",
    "                with open(\n",
    "                    os.path.join(snp_path + Name_experement + \"_lock.csv\"), \"w\"\n",
    "                ) as rez_file:\n",
    "                    rez_file.write(\"Epoch,train_loss,train_acc,val_loss,val_acc\\n\")\n",
    "                best_acc = epoch_acc\n",
    "                best_Loss_classification = epoch_classification_loss\n",
    "                best_epoch_acc = 1\n",
    "                best_epoch_classification = 1\n",
    "\n",
    "            # Обновить скорость обучения\n",
    "            if (\n",
    "                phase == \"val\"\n",
    "                and type(scheduler) == torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "            ):\n",
    "                scheduler.step(epoch_acc)\n",
    "            elif (\n",
    "                phase == \"train\"\n",
    "                and type(scheduler) != torch.optim.lr_scheduler.ReduceLROnPlateau\n",
    "            ):\n",
    "                scheduler.step()\n",
    "\n",
    "            if phase == \"train\":\n",
    "\n",
    "                with open(\n",
    "                    os.path.join(snp_path + Name_experement + \"_lock.csv\"), \"a\"\n",
    "                ) as rez_file:\n",
    "                    rez_file.write(\n",
    "                        str(epoch + 1)\n",
    "                        + \",\"\n",
    "                        + str(epoch_classification_loss)\n",
    "                        + \",\"\n",
    "                        + str(epoch_acc)\n",
    "                    )\n",
    "            else:\n",
    "                with open(\n",
    "                    os.path.join(snp_path + Name_experement + \"_lock.csv\"), \"a\"\n",
    "                ) as rez_file:\n",
    "                    rez_file.write(\n",
    "                        \",\"\n",
    "                        + str(round(epoch_classification_loss, 4))\n",
    "                        + \",\"\n",
    "                        + str(round(epoch_acc, 4))\n",
    "                        + \"\\n\"\n",
    "                    )\n",
    "\n",
    "                mass[0].append(epoch)\n",
    "                mass[1].append(epoch_classification_loss)\n",
    "                mass[2].append(epoch_acc)\n",
    "\n",
    "            # Копироование весов успешной модели на вэйле\n",
    "            if phase == \"val\" and best_acc < epoch_acc:\n",
    "                if epoch_classification_loss < best_Loss_classification:\n",
    "                    best_Loss_classification = epoch_classification_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_epoch_acc = epoch + 1\n",
    "                best_model_the_acc_classification = model.state_dict()\n",
    "                save_name = (\n",
    "                    snp_path\n",
    "                    + Name_experement\n",
    "                    + \"_\"\n",
    "                    + str(epoch + 1)\n",
    "                    + \"_ACC top1-\"\n",
    "                    + str(round(best_acc, 4))\n",
    "                    + \"_checkpoint.tar\"\n",
    "                )\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"epoch\": epoch + 1,\n",
    "                        \"state_dict\": model.state_dict(),\n",
    "                    },\n",
    "                    save_name,\n",
    "                )\n",
    "                print(\"Best val Acc classification:{:4f}\".format(best_acc))\n",
    "            elif (\n",
    "                phase == \"val\" and epoch_classification_loss < best_Loss_classification\n",
    "            ):\n",
    "                best_Loss_classification = epoch_classification_loss\n",
    "                best_epoch_classification = epoch + 1\n",
    "                best_model_the_loss_classification = model.state_dict()\n",
    "                save_name = (\n",
    "                    snp_path\n",
    "                    + Name_experement\n",
    "                    + \"_\"\n",
    "                    + str(epoch + 1)\n",
    "                    + \"_CrossEntropyLoss-\"\n",
    "                    + str(round(best_Loss_classification, 4))\n",
    "                    + \"_checkpoint.tar\"\n",
    "                )\n",
    "                # torch.save({\n",
    "                #             'epoch': epoch + 1,\n",
    "                #             'state_dict': model.state_dict(),\n",
    "                #             }, save_name)\n",
    "                print(\n",
    "                    \"Best Loss classification: {:4f}\".format(best_Loss_classification)\n",
    "                )\n",
    "\n",
    "    # Конечное время и печать времени работы\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training complete in {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Best val Loss classification: {:.4f} epoch {:.0f}  \".format(\n",
    "            best_Loss_classification, best_epoch_classification\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Best val Loss accuracy: {:.4f} epoch {:.0f}\".format(best_acc, best_epoch_acc)\n",
    "    )\n",
    "\n",
    "    show(mass, 0, len(mass[1]))\n",
    "    overfit_model = model\n",
    "    modelLoss = copy.deepcopy(model)\n",
    "    modelAcc = copy.deepcopy(model)\n",
    "    modelLoss.load_state_dict(best_model_the_loss_classification)\n",
    "    modelAcc.load_state_dict(best_model_the_acc_classification)\n",
    "    return modelAcc, modelLoss, overfit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(list, platform):\n",
    "    for i in range(len(list)):\n",
    "        if list[i] == platform:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_model(model_name, N_class=256, path=None, model_old=None, pretrained=False):\n",
    "    model_list_names = timm.list_models(pretrained=pretrained)\n",
    "    if search(model_list_names, model_name):\n",
    "        model = timm.create_model(\n",
    "            model_name, pretrained=pretrained, num_classes=N_class\n",
    "        )  # ,drop_rate=0.2)\n",
    "    else:\n",
    "        print(\"Модель не найдена\")\n",
    "\n",
    "    devices = torch.device(\"cuda:0\" if is_available() else \"cpu\")\n",
    "    model = model.to(devices)\n",
    "\n",
    "    if path != None:\n",
    "        weights = load(path)\n",
    "        model.load_state_dict(weights[\"state_dict\"], strict=True)\n",
    "        model = model.eval()\n",
    "    elif model_old != None:\n",
    "        weights = model_old.state_dict()\n",
    "        model.load_state_dict(weights, strict=True)\n",
    "        model = model.eval()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = get_model(model_name, N_class, model_path, pretrained=pretrained)\n",
    "model_ft = model_ft.to(torch.device(\"cuda:0\" if is_available() else \"cpu\"))\n",
    "model_ft.eval()\n",
    "optimizer_ft = optim.AdamW(\n",
    "    model_ft.parameters(),\n",
    "    lr=lr,\n",
    ")  # betas=(momentum,momentumB),)\n",
    "exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, \"max\", patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name_experement = model_name + \"_\" + Dataset_name\n",
    "snp_path = snp_path_0 + Dataset_name + \"/\" + model_name + \"/\"\n",
    "os.makedirs(snp_path, exist_ok=True)\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d_%m_%Y\")\n",
    "tm_string = now.strftime(\"/%H_%M_%S/\")\n",
    "snp_path = snp_path + dt_string + tm_string\n",
    "os.makedirs(snp_path)\n",
    "print(snp_path)\n",
    "\n",
    "model1, model2, overfit_model = train_model(\n",
    "    model_ft,\n",
    "    classification_criterion,\n",
    "    optimizer_ft,\n",
    "    dataloaders,\n",
    "    exp_lr_scheduler,\n",
    "    batch_size,\n",
    "    snp_path,\n",
    "    Name_experement,\n",
    "    num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224, device=\"cuda\")\n",
    "input_names = [\"actual_input\"] + [\"learned_%d\" % i for i in range(16)]\n",
    "# torch.onnx.export(model_ft, dummy_input, './model_simple.onnx',opset_version=13, input_names=input_names, verbose=True,output_names=['output'])\n",
    "# torch.onnx.export(model, dummy_input, snp+ Name_pruning + \"_\" + modelName + \"_iter_{}_acc_{:.3f}.onnx\".format(i+1, acc), verbose=True, input_names=input_names, output_names=output_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c05aa4aacc3846fa4cd34ace5ac6047d6b88a1f81619af340efafced5e57c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
